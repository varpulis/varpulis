#!/usr/bin/env python3
"""
E2E Raft Consensus Cluster Test Driver for Varpulis CEP.

Tests Raft leader election, cross-coordinator state replication,
cross-coordinator pipeline migration, and leader failover with real
Docker containers, 3-node Raft cluster, and MQTT traffic.

Architecture:
  - 3 Raft coordinators (coordinator-1=node1, coordinator-2=node2, coordinator-3=node3)
  - 4 workers: workers 1+2 -> coordinator-1, worker-3 -> coordinator-2, worker-4 -> coordinator-3
  - 4 pipelines (one per worker by default)
  - Each pipeline reads from a separate MQTT input topic
  - All pipelines write to a shared output topic

Test phases:
  0. Raft Formation  -- leader elected, all coordinators healthy, 4 workers registered
  1. State Replication -- deploy on leader, verify followers see state
  2. Baseline Traffic -- all 4 pipelines processing
  3. Worker Failover  -- kill worker-1, verify pipeline migration
  4. Cross-Coordinator Migration -- kill all coord-1 workers, pipelines move to coord-2/3
  5. Leader Failover  -- kill Raft leader, new leader elected, API available
  6. Recovery         -- restart killed coordinator, rejoins as follower
  7. Traffic During Leader Kill -- continuous traffic, kill leader, verify delivery
"""

import json
import os
import sys
import threading
import time
import traceback

import docker
import paho.mqtt.client as mqtt
import requests

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

COORDINATOR_URLS = [
    os.environ.get("COORDINATOR_1_URL", "http://coordinator-1:9100"),
    os.environ.get("COORDINATOR_2_URL", "http://coordinator-2:9100"),
    os.environ.get("COORDINATOR_3_URL", "http://coordinator-3:9100"),
]

MQTT_HOST = os.environ.get("MQTT_HOST", "mosquitto")
MQTT_PORT = int(os.environ.get("MQTT_PORT", "1883"))

INPUT_TOPIC_PREFIX = "e2e/input"
OUTPUT_TOPIC = "e2e/output"
NUM_PIPELINES = 4

POLL_INTERVAL = 2.0
FAILOVER_TIMEOUT = 45.0
EVENT_WAIT_TIMEOUT = 30.0

# ---------------------------------------------------------------------------
# VPL template
# ---------------------------------------------------------------------------

VPL_TEMPLATE = """
event SensorReading:
    sensor_id: str
    temperature: float
    seq: int

stream Input = SensorReading
    .from(mqtt_{n}, topic: "{input_topic}")

stream HighTemp = Input
    .where(temperature > 50.0)

stream Output = HighTemp
    .to(mqtt_{n}, topic: "e2e/output")
""".strip()


def vpl_for_pipeline(n):
    return VPL_TEMPLATE.format(n=n, input_topic=f"{INPUT_TOPIC_PREFIX}/{n}")


# ---------------------------------------------------------------------------
# MQTT helpers
# ---------------------------------------------------------------------------

class MqttCollector:
    def __init__(self, host, port, topic):
        self.host = host
        self.port = port
        self.topic = topic
        self._events = []
        self._lock = threading.Lock()
        self.client = mqtt.Client(mqtt.CallbackAPIVersion.VERSION2)
        self.client.on_connect = self._on_connect
        self.client.on_message = self._on_message
        self._connected = threading.Event()

    def _on_connect(self, client, userdata, flags, rc, properties=None):
        if rc == 0:
            client.subscribe(self.topic)
            self._connected.set()

    def _on_message(self, client, userdata, msg):
        try:
            data = json.loads(msg.payload.decode())
            with self._lock:
                self._events.append(data)
        except Exception:
            pass

    def start(self):
        self.client.connect(self.host, self.port, 60)
        self.client.loop_start()
        if not self._connected.wait(timeout=10):
            raise RuntimeError("MQTT collector failed to connect")

    def stop(self):
        self.client.loop_stop()
        self.client.disconnect()

    def events(self):
        with self._lock:
            return list(self._events)

    def clear(self):
        with self._lock:
            self._events.clear()

    def count(self):
        with self._lock:
            return len(self._events)

    def wait_for_count(self, expected, timeout=EVENT_WAIT_TIMEOUT):
        deadline = time.time() + timeout
        while time.time() < deadline:
            if self.count() >= expected:
                return True
            time.sleep(0.5)
        return False


class MqttPublisher:
    def __init__(self, host, port):
        self.host = host
        self.port = port
        self.client = mqtt.Client(mqtt.CallbackAPIVersion.VERSION2)

    def connect(self):
        self.client.connect(self.host, self.port, 60)
        self.client.loop_start()

    def disconnect(self):
        self.client.loop_stop()
        self.client.disconnect()

    def publish_event(self, topic, sensor_id, temperature, seq):
        payload = json.dumps({
            "event_type": "SensorReading",
            "sensor_id": sensor_id,
            "temperature": temperature,
            "seq": seq,
        })
        self.client.publish(topic, payload)

    def publish_batch(self, start_seq, count, num_pipelines=NUM_PIPELINES):
        matching = 0
        for i in range(count):
            seq = start_seq + i
            topic_n = (i % num_pipelines) + 1
            temp = 75.0 if seq % 2 == 0 else 25.0
            self.publish_event(
                f"{INPUT_TOPIC_PREFIX}/{topic_n}",
                f"sensor_{seq % 10}",
                temp,
                seq,
            )
            if temp > 50.0:
                matching += 1
        return matching


# ---------------------------------------------------------------------------
# Coordinator + Raft API helpers
# ---------------------------------------------------------------------------

def api_get(path, coordinator_url):
    resp = requests.get(f"{coordinator_url}{path}", timeout=10)
    resp.raise_for_status()
    return resp.json()


def api_post(path, data=None, coordinator_url=None):
    resp = requests.post(f"{coordinator_url}{path}", json=data or {}, timeout=10)
    resp.raise_for_status()
    return resp.json()


def leader_post(path, data=None):
    """Post to the Raft leader. Retries on 421 (Misdirected) by finding leader."""
    # Try each coordinator; on 421, find the leader and retry there
    last_err = None
    for url in COORDINATOR_URLS:
        try:
            return api_post(path, data, coordinator_url=url)
        except requests.exceptions.HTTPError as e:
            if e.response is not None and e.response.status_code == 421:
                last_err = e
                continue
            raise
    # All returned 421; find leader explicitly and try once more
    _, leader_url = find_raft_leader()
    if leader_url:
        return api_post(path, data, coordinator_url=leader_url)
    raise last_err or RuntimeError("No Raft leader found")


def coordinator_health(coordinator_url):
    try:
        return api_get("/health", coordinator_url=coordinator_url)
    except Exception:
        return None


def coordinator_is_up(coordinator_url):
    return coordinator_health(coordinator_url) is not None


def raft_metrics(coordinator_url):
    """Get Raft metrics from a coordinator. Returns None on failure."""
    try:
        return api_get("/raft/metrics", coordinator_url=coordinator_url)
    except Exception:
        return None


def find_raft_leader():
    """Find the current Raft leader across all coordinators.

    Returns (leader_node_id, leader_url) or (None, None).
    """
    for url in COORDINATOR_URLS:
        m = raft_metrics(url)
        if m and m.get("current_leader") not in (None, "null"):
            leader_id = int(m["current_leader"])
            if 1 <= leader_id <= len(COORDINATOR_URLS):
                return leader_id, COORDINATOR_URLS[leader_id - 1]
    return None, None


def wait_for_raft_leader(timeout=30, exclude_node=None):
    """Wait until a Raft leader is elected. Returns (leader_id, leader_url)."""
    deadline = time.time() + timeout
    while time.time() < deadline:
        leader_id, leader_url = find_raft_leader()
        if leader_id is not None:
            if exclude_node is None or leader_id != exclude_node:
                return leader_id, leader_url
        time.sleep(1)
    raise TimeoutError(f"No Raft leader elected within {timeout}s")


def get_raft_role(coordinator_url):
    """Get Raft role: 'Leader', 'Follower', 'Candidate', or 'Unknown'."""
    m = raft_metrics(coordinator_url)
    if m:
        return m.get("state", "Unknown")
    return "Unreachable"


# ---------------------------------------------------------------------------
# Docker helpers
# ---------------------------------------------------------------------------

def get_docker_client():
    return docker.from_env()


def find_container(docker_client, service_name):
    containers = docker_client.containers.list(
        all=True,
        filters={"label": f"com.docker.compose.service={service_name}"},
    )
    return containers[0] if containers else None


def stop_container(docker_client, service_name):
    container = find_container(docker_client, service_name)
    if container:
        container.stop(timeout=0)
        print(f"    Stopped container for {service_name}")
        return True
    print(f"    WARNING: container for {service_name} not found")
    return False


def start_container(docker_client, service_name):
    container = find_container(docker_client, service_name)
    if container:
        container.start()
        print(f"    Started container for {service_name}")
        return True
    print(f"    WARNING: container for {service_name} not found")
    return False


def get_container_logs(docker_client, service_name, tail=50):
    container = find_container(docker_client, service_name)
    if container:
        return container.logs(tail=tail).decode("utf-8", errors="replace")
    return ""


# ---------------------------------------------------------------------------
# Polling helpers
# ---------------------------------------------------------------------------

def wait_for_workers(expected_ready, timeout=60, coordinator_url=None):
    deadline = time.time() + timeout
    while time.time() < deadline:
        try:
            data = api_get("/api/v1/cluster/workers", coordinator_url=coordinator_url)
            ready = [
                w for w in data.get("workers", [])
                if w.get("status", "").lower() == "ready"
            ]
            if len(ready) >= expected_ready:
                return ready
        except Exception:
            pass
        time.sleep(POLL_INTERVAL)
    raise TimeoutError(f"Timed out waiting for {expected_ready} ready workers on {coordinator_url}")


def wait_for_worker_status(worker_id, statuses, timeout=FAILOVER_TIMEOUT,
                           coordinator_url=None):
    statuses_lower = [s.lower() for s in statuses]
    deadline = time.time() + timeout
    while time.time() < deadline:
        try:
            data = api_get("/api/v1/cluster/workers", coordinator_url=coordinator_url)
            for w in data.get("workers", []):
                wid = w.get("id") or w.get("worker_id", "")
                if wid == worker_id:
                    if w.get("status", "").lower() in statuses_lower:
                        return True
        except Exception:
            pass
        time.sleep(POLL_INTERVAL)
    return False


def get_topology_pipelines(coordinator_url):
    topo = api_get("/api/v1/cluster/topology", coordinator_url=coordinator_url)
    pipelines = []
    for group in topo.get("groups", []):
        pipelines.extend(group.get("pipelines", []))
    return pipelines


def wait_for_all_pipelines_assigned(expected_count, exclude_workers=None,
                                     timeout=FAILOVER_TIMEOUT,
                                     coordinator_url=None):
    exclude_workers = set(exclude_workers or [])
    deadline = time.time() + timeout
    while time.time() < deadline:
        try:
            pipelines = get_topology_pipelines(coordinator_url=coordinator_url)
            if len(pipelines) >= expected_count:
                assigned_workers = {p["worker_id"] for p in pipelines}
                if not assigned_workers & exclude_workers:
                    return pipelines
        except Exception:
            pass
        time.sleep(POLL_INTERVAL)
    raise TimeoutError(
        f"Not all {expected_count} pipelines reassigned within {timeout}s "
        f"(excluding {exclude_workers})"
    )


def wait_for_migrations_complete(timeout=FAILOVER_TIMEOUT, coordinator_url=None):
    deadline = time.time() + timeout
    while time.time() < deadline:
        try:
            data = api_get("/api/v1/cluster/migrations", coordinator_url=coordinator_url)
            active = [
                m for m in data.get("migrations", [])
                if m.get("status", "").lower() in ("pending", "inprogress", "in_progress")
            ]
            if not active:
                return True
        except Exception:
            pass
        time.sleep(POLL_INTERVAL)
    return False


# ---------------------------------------------------------------------------
# Phase result
# ---------------------------------------------------------------------------

class PhaseResult:
    def __init__(self, name, passed, duration, detail=""):
        self.name = name
        self.passed = passed
        self.duration = duration
        self.detail = detail


# ---------------------------------------------------------------------------
# Test phases
# ---------------------------------------------------------------------------

def phase_0_raft_formation(docker_client):
    """Raft cluster formation: leader elected, all coordinators healthy, workers registered."""
    start = time.time()

    # 1. Check all 3 coordinators are healthy
    print("  Checking all 3 coordinators are healthy...")
    all_healthy = True
    for i, url in enumerate(COORDINATOR_URLS, 1):
        health = coordinator_health(url)
        if health:
            print(f"  Coordinator-{i}: status={health.get('status')}, workers={health.get('workers', {}).get('total', 0)}")
        else:
            print(f"  Coordinator-{i}: UNREACHABLE")
            all_healthy = False

    # 2. Wait for Raft leader election
    print("  Waiting for Raft leader election...")
    leader_id, leader_url = wait_for_raft_leader(timeout=30)
    print(f"  Raft leader: node {leader_id} ({leader_url})")

    # 3. Check all nodes have Raft roles
    roles = {}
    for i, url in enumerate(COORDINATOR_URLS, 1):
        role = get_raft_role(url)
        roles[i] = role
        print(f"  Coordinator-{i} Raft role: {role}")

    leader_count = sum(1 for r in roles.values() if "Leader" in r)
    follower_count = sum(1 for r in roles.values() if "Follower" in r)

    # 4. Wait for all 4 workers visible on any coordinator (Raft replicates state)
    print("  Waiting for workers...")
    try:
        workers = wait_for_workers(4, timeout=30, coordinator_url=leader_url)
        total_workers = len(workers)
        print(f"  Leader sees {total_workers} workers ready")
    except TimeoutError:
        total_workers = 0
        print("  TIMEOUT waiting for 4 workers on leader")

    # Verify followers also see the workers (via Raft replication)
    for i, url in enumerate(COORDINATOR_URLS, 1):
        if i == leader_id:
            continue
        try:
            fw = wait_for_workers(4, timeout=10, coordinator_url=url)
            print(f"  Coordinator-{i} (follower): {len(fw)} workers visible")
        except TimeoutError:
            print(f"  Coordinator-{i} (follower): fewer than 4 workers visible")

    duration = time.time() - start
    passed = (
        all_healthy
        and leader_count == 1
        and follower_count == 2
        and total_workers >= 4
    )
    detail = (f"leader=node {leader_id}, "
              f"{leader_count} leader + {follower_count} followers, "
              f"{total_workers} workers")
    return PhaseResult("Raft Formation", passed, duration, detail)


def phase_1_state_replication(publisher, collector, docker_client):
    """Deploy pipelines via leader, verify followers see the state via Raft."""
    start = time.time()

    leader_id, leader_url = find_raft_leader()
    print(f"  Leader: node {leader_id} ({leader_url})")

    # Register MQTT connectors via leader (Raft replicates to followers)
    print("  Registering MQTT connectors via leader...")
    for n in range(1, NUM_PIPELINES + 1):
        try:
            leader_post("/api/v1/cluster/connectors", {
                "name": f"mqtt_{n}",
                "connector_type": "mqtt",
                "params": {"host": MQTT_HOST, "port": str(MQTT_PORT)},
                "description": f"MQTT broker for pipeline {n}",
            })
        except requests.exceptions.HTTPError as e:
            if e.response is not None and e.response.status_code == 409:
                pass  # already exists
            else:
                raise

    # Deploy all pipeline groups via the leader
    print("  Deploying pipeline groups via leader...")
    deploy_configs = [
        ("e2e_group_c1", [1, 2]),
        ("e2e_group_c2", [3]),
        ("e2e_group_c3", [4]),
    ]
    for group_name, pipeline_nums in deploy_configs:
        try:
            leader_post("/api/v1/cluster/pipeline-groups", {
                "name": group_name,
                "pipelines": [
                    {"name": f"sensor_filter_{n}", "source": vpl_for_pipeline(n), "replicas": 1}
                    for n in pipeline_nums
                ],
                "routes": [],
            })
            print(f"    Deployed {group_name}")
        except requests.exceptions.HTTPError as e:
            if e.response is not None and e.response.status_code == 409:
                print(f"    {group_name} already exists")
            else:
                raise

    # Wait for deployment
    print("  Waiting for pipelines to deploy...")
    time.sleep(5)

    # Verify state replication: all coordinators should see the same topology
    leader_pipelines = 0
    for i, url in enumerate(COORDINATOR_URLS, 1):
        try:
            pipelines = get_topology_pipelines(coordinator_url=url)
            worker_set = {p["worker_id"] for p in pipelines}
            print(f"  Coordinator-{i}: {len(pipelines)} pipelines on {worker_set}")
            if i == leader_id:
                leader_pipelines = len(pipelines)
        except Exception as e:
            print(f"  Coordinator-{i}: topology check failed: {e}")

    # Warm up with events
    print("  Sending warm-up events...")
    expected = publisher.publish_batch(start_seq=-100, count=100)
    time.sleep(5)

    warmup_count = collector.count()
    print(f"  Warm-up: received {warmup_count}/{expected} events")
    collector.clear()

    duration = time.time() - start
    passed = leader_pipelines >= NUM_PIPELINES and warmup_count > 0
    detail = f"{leader_pipelines} pipelines deployed, warm-up {warmup_count}/{expected}"
    return PhaseResult("State Replication", passed, duration, detail)


def phase_2_baseline(publisher, collector):
    """Baseline: all 4 pipelines, 500 events (250 matching)."""
    start = time.time()
    collector.clear()

    expected = publisher.publish_batch(start_seq=0, count=500)
    print(f"  Published 500 events ({expected} matching)")

    collector.wait_for_count(expected, timeout=30)
    received = collector.count()

    duration = time.time() - start
    passed = received >= expected
    detail = f"{received}/{expected} events"
    return PhaseResult("Baseline", passed, duration, detail)


def phase_3_worker_failover(publisher, collector, docker_client):
    """Kill worker-1, verify failover detection and pipeline migration."""
    start = time.time()
    collector.clear()

    leader_id, leader_url = find_raft_leader()

    # Kill worker-1
    print("  Killing worker-1...")
    stop_container(docker_client, "worker-1")

    # Wait for failover detection on coordinator-1 (worker-1's WS home)
    print("  Waiting for failover detection...")
    failover_start = time.time()
    wait_for_worker_status("worker-1", ["unhealthy", "dead"],
                           timeout=FAILOVER_TIMEOUT,
                           coordinator_url=COORDINATOR_URLS[0])
    detect_time = time.time() - failover_start
    print(f"  Worker-1 marked unhealthy after {detect_time:.1f}s")

    # Wait for pipeline migration away from worker-1
    # With Raft, all coordinators see all 4 pipelines; check that at least
    # the original 4 are assigned and none are on worker-1
    print("  Waiting for pipeline migration...")
    pipelines = wait_for_all_pipelines_assigned(
        NUM_PIPELINES, exclude_workers=["worker-1"], timeout=FAILOVER_TIMEOUT,
        coordinator_url=leader_url,
    )
    failover_time = time.time() - failover_start
    print(f"  Failover completed in {failover_time:.1f}s")

    worker_set = {p["worker_id"] for p in pipelines}
    assert "worker-1" not in worker_set, "worker-1 still has pipelines"
    print(f"  All pipelines now on: {worker_set}")

    # Send events after migration (to all 4 topics, but only 3 workers active)
    expected = publisher.publish_batch(start_seq=500, count=500)
    print(f"  Published 500 events ({expected} matching)")

    collector.wait_for_count(expected, timeout=30)
    received = collector.count()

    duration = time.time() - start
    passed = received >= expected and "worker-1" not in worker_set
    detail = (f"{received}/{expected} events, failover in {failover_time:.1f}s, "
              f"pipelines on {worker_set}")
    return PhaseResult("Worker Failover", passed, duration, detail)


def phase_4_cross_coordinator_migration(publisher, collector, docker_client):
    """Kill all coordinator-1 workers, verify pipelines migrate cross-coordinator."""
    start = time.time()
    collector.clear()

    # Kill worker-2 (the only remaining worker on coordinator-1)
    # worker-1 is already dead from phase 3
    print("  Killing worker-2 (last worker on coordinator-1)...")
    stop_container(docker_client, "worker-2")

    print("  Waiting for worker-2 failover detection...")
    failover_start = time.time()
    wait_for_worker_status("worker-2", ["unhealthy", "dead"],
                           timeout=FAILOVER_TIMEOUT,
                           coordinator_url=COORDINATOR_URLS[0])
    detect_time = time.time() - failover_start
    print(f"  Worker-2 marked unhealthy after {detect_time:.1f}s")

    # Coordinator-1 has NO workers now
    # Pipelines 1+2 should eventually be detected as unassigned
    # But since coordinators don't share workers without Raft cross-migration yet,
    # we verify that coordinator-2 and coordinator-3 are still operational

    print("  Verifying coordinator-2 and coordinator-3 are operational...")
    health_2 = coordinator_health(COORDINATOR_URLS[1])
    health_3 = coordinator_health(COORDINATOR_URLS[2])
    coord_2_ok = health_2 is not None
    coord_3_ok = health_3 is not None
    print(f"  Coordinator-2: {health_2.get('status') if health_2 else 'UNREACHABLE'}")
    print(f"  Coordinator-3: {health_3.get('status') if health_3 else 'UNREACHABLE'}")

    # Send events to pipelines 3+4 (on coordinator-2/3 â€” still have workers)
    print("  Publishing events to pipelines 3+4 (surviving coordinators)...")
    collector.clear()
    matching = 0
    for i in range(500):
        seq = 1000 + i
        topic_n = 3 if i % 2 == 0 else 4
        temp = 75.0 if seq % 2 == 0 else 25.0
        publisher.publish_event(f"{INPUT_TOPIC_PREFIX}/{topic_n}", f"sensor_{seq % 10}", temp, seq)
        if temp > 50.0:
            matching += 1

    collector.wait_for_count(matching, timeout=30)
    received = collector.count()

    duration = time.time() - start
    passed = received >= matching and coord_2_ok and coord_3_ok
    detail = (f"{received}/{matching} events via coord-2/3, "
              f"coord-2 ok: {coord_2_ok}, coord-3 ok: {coord_3_ok}")
    return PhaseResult("Cross-Coordinator Migration", passed, duration, detail)


def phase_5_leader_failover(publisher, collector, docker_client):
    """Kill the Raft leader, verify new leader elected and API available."""
    start = time.time()
    collector.clear()

    # Find current leader
    old_leader_id, old_leader_url = find_raft_leader()
    print(f"  Current Raft leader: node {old_leader_id} ({old_leader_url})")

    # Kill the leader
    leader_service = f"coordinator-{old_leader_id}"
    print(f"  KILLING {leader_service} (Raft leader)...")
    stop_container(docker_client, leader_service)

    time.sleep(2)
    # Verify leader is down
    leader_down = not coordinator_is_up(old_leader_url)
    print(f"  {leader_service} is down: {leader_down}")

    # Wait for new leader election among survivors
    print("  Waiting for new Raft leader election...")
    election_start = time.time()
    try:
        new_leader_id, new_leader_url = wait_for_raft_leader(
            timeout=30, exclude_node=old_leader_id)
        election_time = time.time() - election_start
        print(f"  New leader: node {new_leader_id} ({new_leader_url}) "
              f"elected in {election_time:.1f}s")
    except TimeoutError:
        new_leader_id, new_leader_url = None, None
        election_time = time.time() - election_start
        print(f"  TIMEOUT: No new leader after {election_time:.1f}s")

    # Check surviving coordinators' Raft roles
    for i, url in enumerate(COORDINATOR_URLS, 1):
        if i == old_leader_id:
            continue
        role = get_raft_role(url)
        print(f"  Coordinator-{i} role: {role}")

    # Verify API is available on surviving coordinators
    # (at least one coordinator with workers should still serve pipelines)
    surviving_urls = [url for i, url in enumerate(COORDINATOR_URLS, 1) if i != old_leader_id]
    api_available = False
    for url in surviving_urls:
        h = coordinator_health(url)
        if h is not None:
            api_available = True
            print(f"  API available on {url}: status={h.get('status')}")

    # With Raft, pipelines are distributed across all workers.
    # Workers 1+2 are dead from phases 3+4; workers 3+4 are alive.
    # Pipelines on alive workers should still process events.
    # Query topology from a surviving coordinator to find active pipelines.
    test_pipelines = []
    for url in surviving_urls:
        try:
            pipelines = get_topology_pipelines(coordinator_url=url)
            alive_workers = {"worker-3", "worker-4"}
            for p in pipelines:
                wid = p.get("worker_id", "")
                pname = p.get("name", "")
                if wid in alive_workers:
                    # Extract pipeline number from name like "sensor_filter_3"
                    for n in range(1, NUM_PIPELINES + 1):
                        if f"sensor_filter_{n}" in pname:
                            test_pipelines.append(n)
                            break
            break
        except Exception:
            continue
    if not test_pipelines:
        test_pipelines = [3, 4]  # fallback

    if test_pipelines:
        print(f"  Publishing events to pipelines {test_pipelines}...")
        collector.clear()
        matching = 0
        for i in range(200):
            seq = 2000 + i
            topic_n = test_pipelines[i % len(test_pipelines)]
            temp = 75.0 if seq % 2 == 0 else 25.0
            publisher.publish_event(f"{INPUT_TOPIC_PREFIX}/{topic_n}", f"sensor_{seq % 10}", temp, seq)
            if temp > 50.0:
                matching += 1

        collector.wait_for_count(matching, timeout=20)
        received = collector.count()
        print(f"  Received {received}/{matching} events after leader failover")
    else:
        received = 0
        matching = 0
        print("  No surviving pipelines with live workers to test")

    duration = time.time() - start
    passed = (
        leader_down
        and new_leader_id is not None
        and new_leader_id != old_leader_id
        and api_available
        and (received >= matching if matching > 0 else True)
    )
    detail = (f"old leader: node {old_leader_id}, "
              f"new leader: node {new_leader_id}, "
              f"election: {election_time:.1f}s, "
              f"events: {received}/{matching}")
    return PhaseResult("Leader Failover", passed, duration, detail)


def phase_6_recovery(publisher, collector, docker_client):
    """Restart killed coordinators and workers, verify cluster recovery."""
    start = time.time()
    collector.clear()

    # Find what was killed
    old_leader_id, _ = None, None
    for i, url in enumerate(COORDINATOR_URLS, 1):
        if not coordinator_is_up(url):
            old_leader_id = i
            break

    # Restart the killed coordinator
    if old_leader_id:
        print(f"  Restarting coordinator-{old_leader_id}...")
        start_container(docker_client, f"coordinator-{old_leader_id}")

        # Wait for it to come back
        deadline = time.time() + 30
        while time.time() < deadline:
            if coordinator_is_up(COORDINATOR_URLS[old_leader_id - 1]):
                print(f"  Coordinator-{old_leader_id} is back!")
                break
            time.sleep(1)
        else:
            print(f"  WARNING: coordinator-{old_leader_id} did not recover within 30s")

    # Restart workers 1 and 2
    print("  Restarting worker-1 and worker-2...")
    start_container(docker_client, "worker-1")
    start_container(docker_client, "worker-2")

    # Wait for workers to register with coordinator-1
    print("  Waiting for workers to register...")
    try:
        workers = wait_for_workers(2, timeout=60, coordinator_url=COORDINATOR_URLS[0])
        print(f"  {len(workers)} workers ready on coordinator-1")
    except TimeoutError:
        print("  WARNING: workers did not re-register within 60s")

    # Check Raft roles after recovery
    print("  Raft roles after recovery:")
    for i, url in enumerate(COORDINATOR_URLS, 1):
        role = get_raft_role(url)
        print(f"  Coordinator-{i}: {role}")

    # Verify we have a leader
    leader_id, leader_url = find_raft_leader()
    has_leader = leader_id is not None
    print(f"  Raft leader: node {leader_id}")

    # Re-register connectors via leader (state may be lost without persistent storage)
    print("  Re-registering MQTT connectors via leader...")
    for n in range(1, NUM_PIPELINES + 1):
        try:
            leader_post("/api/v1/cluster/connectors", {
                "name": f"mqtt_{n}",
                "connector_type": "mqtt",
                "params": {"host": MQTT_HOST, "port": str(MQTT_PORT)},
                "description": f"MQTT broker for pipeline {n}",
            })
        except requests.exceptions.HTTPError:
            pass  # already exists or no leader

    # Re-deploy pipeline groups via leader if needed
    print("  Re-deploying pipeline groups via leader if needed...")
    for group_name, pipeline_nums in [
        ("e2e_group_c1", [1, 2]),
        ("e2e_group_c2", [3]),
        ("e2e_group_c3", [4]),
    ]:
        try:
            leader_post("/api/v1/cluster/pipeline-groups", {
                "name": group_name,
                "pipelines": [
                    {"name": f"sensor_filter_{n}", "source": vpl_for_pipeline(n), "replicas": 1}
                    for n in pipeline_nums
                ],
                "routes": [],
            })
        except requests.exceptions.HTTPError:
            pass  # already exists

    time.sleep(5)

    # Verify all coordinators are up
    all_up = all(coordinator_is_up(url) for url in COORDINATOR_URLS)
    print(f"  All 3 coordinators up: {all_up}")

    # Send events across all pipelines
    collector.clear()
    expected = publisher.publish_batch(start_seq=3000, count=500)
    print(f"  Published 500 events ({expected} matching)")

    collector.wait_for_count(expected, timeout=30)
    received = collector.count()

    duration = time.time() - start
    passed = received >= expected and all_up and has_leader
    detail = (f"{received}/{expected} events, all coordinators up: {all_up}, "
              f"leader: node {leader_id}")
    return PhaseResult("Recovery", passed, duration, detail)


def phase_7_traffic_during_leader_kill(publisher, collector, docker_client):
    """Continuous MQTT traffic while killing Raft leader."""
    start = time.time()
    collector.clear()

    leader_id, leader_url = find_raft_leader()
    print(f"  Current leader: node {leader_id}")

    stop_flag = threading.Event()
    counters = {"total": 0, "matching": 0, "seq": 5000}

    def background_publish():
        while not stop_flag.is_set():
            seq = counters["seq"]
            topic_n = (seq % NUM_PIPELINES) + 1
            temp = 75.0 if seq % 2 == 0 else 25.0
            publisher.publish_event(
                f"{INPUT_TOPIC_PREFIX}/{topic_n}",
                f"sensor_{seq % 10}",
                temp,
                seq,
            )
            counters["total"] += 1
            if temp > 50.0:
                counters["matching"] += 1
            counters["seq"] += 1
            time.sleep(0.01)  # ~100 events/sec

    print("  Starting background traffic...")
    pub_thread = threading.Thread(target=background_publish, daemon=True)
    pub_thread.start()

    # Publish for 5s, then kill the leader
    time.sleep(5)
    print(f"  Killing coordinator-{leader_id} (Raft leader) during traffic...")
    stop_container(docker_client, f"coordinator-{leader_id}")

    # Continue publishing for 20s
    time.sleep(20)

    # Stop publisher
    stop_flag.set()
    pub_thread.join(timeout=5)

    # Wait for drain
    print("  Waiting for event drain...")
    time.sleep(10)

    total_matching = counters["matching"]
    received = collector.count()
    pct = (received / total_matching * 100) if total_matching > 0 else 0

    print(f"  Total matching events published: {total_matching}")
    print(f"  Received: {received} ({pct:.1f}%)")

    # Check if new leader was elected
    new_leader_id, _ = find_raft_leader()
    has_new_leader = new_leader_id is not None and new_leader_id != leader_id
    print(f"  New leader after kill: node {new_leader_id}")

    # Surviving coordinators should be up
    surviving_ok = 0
    for i, url in enumerate(COORDINATOR_URLS, 1):
        if i == leader_id:
            continue
        if coordinator_is_up(url):
            surviving_ok += 1

    duration = time.time() - start
    # Accept >= 30% delivery (only pipelines with live workers continue)
    passed = pct >= 30.0 and has_new_leader and surviving_ok >= 2
    detail = (f"{received}/{total_matching} events ({pct:.1f}%), "
              f"new leader: node {new_leader_id}, "
              f"{surviving_ok} surviving coordinators OK")
    return PhaseResult("Traffic During Leader Kill", passed, duration, detail)


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    print("=" * 60)
    print("  E2E Raft Consensus Cluster Test Suite")
    print("  Varpulis CEP - 3 Raft coordinators, 4 workers")
    print("=" * 60)
    print()
    for i, url in enumerate(COORDINATOR_URLS, 1):
        print(f"  Coordinator-{i}: {url}")
    print(f"  MQTT:           {MQTT_HOST}:{MQTT_PORT}")
    print()

    docker_client = get_docker_client()
    collector = MqttCollector(MQTT_HOST, MQTT_PORT, OUTPUT_TOPIC)
    publisher = MqttPublisher(MQTT_HOST, MQTT_PORT)

    # Initialize MQTT connections upfront so all phases can use them
    print("Connecting MQTT collector and publisher...")
    collector.start()
    publisher.connect()
    time.sleep(2)
    print("MQTT ready.")
    print()

    results = []

    phases = [
        ("Phase 0", "Raft Formation",
         lambda: phase_0_raft_formation(docker_client)),
        ("Phase 1", "State Replication",
         lambda: phase_1_state_replication(publisher, collector, docker_client)),
        ("Phase 2", "Baseline",
         lambda: phase_2_baseline(publisher, collector)),
        ("Phase 3", "Worker Failover",
         lambda: phase_3_worker_failover(publisher, collector, docker_client)),
        ("Phase 4", "Cross-Coordinator Migration",
         lambda: phase_4_cross_coordinator_migration(publisher, collector, docker_client)),
        ("Phase 5", "Leader Failover",
         lambda: phase_5_leader_failover(publisher, collector, docker_client)),
        ("Phase 6", "Recovery",
         lambda: phase_6_recovery(publisher, collector, docker_client)),
        ("Phase 7", "Traffic During Leader Kill",
         lambda: phase_7_traffic_during_leader_kill(publisher, collector, docker_client)),
    ]

    for phase_id, phase_name, phase_fn in phases:
        print(f"\n--- {phase_id}: {phase_name} ---")
        try:
            result = phase_fn()
            results.append(result)
            status = "PASS" if result.passed else "FAIL"
            print(f"  >> {status} ({result.duration:.1f}s) -- {result.detail}")
        except Exception as e:
            traceback.print_exc()
            results.append(PhaseResult(phase_name, False, 0, str(e)))
            print(f"  >> FAIL -- {e}")

    # Cleanup
    try:
        publisher.disconnect()
    except Exception:
        pass
    try:
        collector.stop()
    except Exception:
        pass

    # Summary
    print()
    print("=" * 60)
    print("=== E2E Raft Consensus Test Results ===")
    for i, r in enumerate(results):
        status = "PASS" if r.passed else "FAIL"
        dots = "." * max(1, 35 - len(r.name))
        print(f"  Phase {i}: {r.name} {dots} {status} ({r.duration:.1f}s)")
        print(f"           {r.detail}")
    passed_count = sum(1 for r in results if r.passed)
    total = len(results)
    print("=" * 60)
    print(f"RESULT: {passed_count}/{total} phases passed")
    print("=" * 60)

    sys.exit(0 if passed_count == total else 1)


if __name__ == "__main__":
    main()
